<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Semi-Humanoid Robot Project by Adam Hazenberg">
    <title>Semi-Humanoid Robot - Adam Hazenberg</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <header>
        <div class="profile-img">
            <img src="images/profile_picture.jpg" alt="Adam Hazenberg">
            <div class="profile-info">
                <h1>Adam Hazenberg</h1>
                <h2>Robotics Engineering Undergrad Student</h2>
            </div>
        </div>
        <nav>
            <a href="index.html">Home</a>
        </nav>
    </header>

    <main>
        <!-- Intro paragraph on its own card -->
        <section class="card">
            <h1>Semi-Humanoid</h1>
            <p>This personal project was both a passion project and an opportunity to acquire new technical skills.
                Since the summer after my first year, I have been developing a human-interaction robot that can
                interpret, respond to, and follow basic commands from humans. The project started as a simple
                overgrown Arduino project but has evolved into a complex system incorporating key concepts of modern
                robotics.</p>
        </section>

        <section class="card">
            <h1>Perception System</h1>
            <p>The challenge for the perception system was to find low-cost solutions for a large variety of sensing
                requirements. For the majority of high-level operations, both object detection and accurate distance
                measurement were necessary. At first, I tried stereo vision, but this proved to be too
                computationally heavy and ineffective at measuring the distance of unidentified objects such as
                obstacles and walls. For this reason, I pivoted to a combination of a single-laser LiDAR sensor
                aligned with a webcam. To lighten the computational load on the Raspberry Pi, I added a Coral TPU
                accelerator. The primary model used for object detection is MobileNetV2.</p>
            <p>Other sensors in the suite/head include:</p>
            <ul>
                <li>GPS and digital compass for outdoor navigation</li>
                <li>IMU for linear actuator zeroing and PID control</li>
                <li>Microphone for speech recognition</li>
            </ul>
            <div class="image-container">
                <img src="images/humanoid_head.png" alt="Head of the Semi-Humanoid Robot">
                <img src="images/humanoid_head_iterations.jpg" alt="Head Iterations of the Semi-Humanoid Robot">
            </div>
        </section>

        <section class="card">
            <h1>Electronics</h1>
            <p>The robot employs a wide variety of sensors, actuators, and computing units:</p>
            <ul>
                <li>Servos (8-11V)</li>
                <li>Geared DC motors (24V)</li>
                <li>Speech synthesis amplifier (12V)</li>
                <li>Linear actuators (12V)</li>
                <li>Arduino Mega microcontroller (12V)</li>
                <li>Raspberry Pi 4 (5V)</li>
                <li>All Sensors (3.3-5V)</li>
            </ul>
            <p>To accommodate the power voltage/amperage requirements for all these components, I designed a centralized
                battery unit that outputs 6V, 12V, and 24V by connecting eight 6V lead-acid batteries in either series
                or parallel. This proved to be an effective alternative to buck converters, which were current-limited.
            </p>
            <div class="image-container">
                <img src="images/humanoid_battery_isometric.png" alt="Battery unit isometric">
                <img src="images/humanoid_battery_diagram.jpg" alt="Battery unit top">
            </div>
            <p>To support the clean, stable 5V/3A required by the Raspberry Pi, a 5V buck converter with a feedback loop
                is used. This ensures that the Raspberry Pi receives a consistent 5V, even when the battery unit is
                partially depleted.
            </p>
            <div class="image-container">
                <img src="images/humanoid_buck_converter.png" style="width: 30%; height: auto;" alt="Buck converter">
            </div>
        </section>

        <section class="card">
            <h1>Mechanical Structure</h1>
            <p>Due to the scale of the robot (1.75m), it was a challenge to design a structure that would minimize
                weight and cost while maintaining rigidity. Additionally, the design needed to be modular to support
                design adjustments (of which there were many). For this reason, T-slot extrusion rails were employed
                for the central structure. Linear actuator leaning joints were CNC-machined from aluminum to prevent
                undermining the rigidity of the rest of the structure.</p>
            <div class="image-container">
                <img src="images/humanoid_full.png" alt="Full Semi-Humanoid Robot">
                <img src="images/humanoid_lean_joints.jpg" alt="Lean Joints">
                <img src="images/humanoid_render.PNG" alt="Render">
            </div>
        </section>

        <section class="card">
            <h1>Notable Capabilities/Subroutines</h1>
            <h3>Grabbing objects</h3>
            <p>The grabbing subroutine makes use of the camera mounted on the gripper to converge the end effector
                towards the object being grabbed. Using forward kinematics, the arm knows its current spatial state,
                and from the pose of the item within the frame, the arm can determine its next desired position. If
                the arm is off in the x or y axis, this slows down the stretching motion proportionately so that the
                item is aligned. The arm then calculates the joint position with inverse kinematics and does this
                repeatedly until an IR sensor is triggered, and the gripper is closed. This seems to work well for
                items less than 12cm in width.</p>
            <div class="image-container">
                <img src="images/humanoid_gripper.png" style="width: 30%; height: auto;" alt="Full Semi-Humanoid Robot">
            </div>
            <h3>Approaching/following objects</h3>
            <p>Another useful subroutine is the ability to approach items after they have been identified. This may
                include following a person or approaching an item to be manipulated. In both cases, the robot tracks
                the desired item with a pan/tilt look-at routine (running in a separate thread) and uses this pose
                estimation to navigate toward the object. This is done by calculating the difference between the
                robot's current heading and the desired heading (toward the object) and feeding this as an error
                into a PID controller. This controller then outputs a wheel speed differential to drive the robot
                towards the target. The robot can then use the LiDAR sensor and neck angle to calculate and
                stop at the specified distance.</p>
            <div class="image-container">
                <video src="videos/humanoid_driving.mp4" alt="Humanoid Driving" autoplay muted loop playsinline>
                    Your browser does not support the video tag.
                </video>
            </div>

            <h3>Area scanning for desired object</h3>
            <p>For any larger routine such as fetching items or finding people, the robot is programmed with a
                function that pans the head in the motion of a sinusoid, while the CV model attempts to find the
                desired item. When the item is detected, the routine will then pan/tilt toward the item to create
                frames that are used to confirm the item with an appearance metric. This routine is 87% accurate in
                good lighting.</p>

            <div class="video-container">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/cuZO3TcMt80?si=Bxcki1RmgtZPq9zk"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            </div>

            <p>All code for the robot can be found on <a href="https://github.com/adamhaze8/Semi-Humanoid-V2"
                    target="_blank">my GitHub page</a></p>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Adam Hazenberg</p>
        <p>
            <a href="mailto:adamhaze8@gmail.com">Email</a> |
            <a href="https://www.linkedin.com/in/adamhazenberg" target="_blank">LinkedIn</a> |
            <a href="https://github.com/adamhaze8" target="_blank">GitHub</a>
        </p>
    </footer>
</body>

</html>